{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloning GitHub Repo\n",
    "\n",
    "This is working by populating a folder called `loaded-repo` in the directory. It is going to populate from a given `repo_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chat.openai.com/c/9840a960-5ff0-459a-b856-5b29033f51dc\n",
    "from git import Repo\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Set environment variable to skip Git LFS files\n",
    "os.environ['GIT_LFS_SKIP_SMUDGE'] = '1'\n",
    "\n",
    "# Define the path where you want to clone the repository\n",
    "repo_path = \"loaded-repo\"  # Replace with your desired local path\n",
    "\n",
    "# Check if the repo_path exists and delete it\n",
    "if os.path.exists(repo_path):\n",
    "    shutil.rmtree(repo_path)\n",
    "\n",
    "# Clone the repository from GitHub\n",
    "repo = Repo.clone_from(\"https://github.com/DataWithAlex/gen3-pokeGAN\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Repository Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# Load python files from the repository\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*.py\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "len(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code snippet, we are utilizing `langchain`, which is a Python library designed for chain-of-thought reasoning and language model workflows. The specific components used here are for loading and parsing documents.\n",
    "\n",
    "1. `GenericLoader.from_filesystem`: This function is part of the `langchain` document loaders. It's designed to load files from the local filesystem. It takes several parameters:\n",
    "   - `repo_path`: The path to the directory where the repository has been cloned. This is where it will look for files to load.\n",
    "   - `glob`: A pattern that specifies which files to include. The `**/*.py` pattern means it will recursively search for all files with a `.py` extension in all subdirectories.\n",
    "   - `suffixes`: This further specifies which file types to include by their extension, in this case, only Python files (`.py`).\n",
    "   - `parser`: This is an instance of `LanguageParser` that is configured to parse Python language. The `parser_threshold` is a setting that can be used to control the parsing behavior, like how much of the file to parse.\n",
    "\n",
    "2. `LanguageParser`: This is a parser from `langchain` document loaders, which is set to interpret and parse the files according to the Python programming language. The `parser_threshold` parameter indicates the size above which files won't be parsed. This is useful for avoiding extremely large files that could be problematic to process.\n",
    "\n",
    "3. `loader.load()`: Once the `GenericLoader` instance is configured, calling `load()` will execute the file loading process according to the specified parameters. It will collect all the Python files within the `repo_path` that match the given pattern and parse them.\n",
    "\n",
    "4. `len(documents)`: After loading the documents, this line simply counts and returns the number of documents (or Python files, in this context) that have been loaded. This gives us an idea of how many Python files are in the repository and have been processed.\n",
    "\n",
    "Overall, this code block is setting up a pipeline to automatically find and load all Python source files from the cloned GitHub repository, which can then be further processed for tasks such as analysis, summarization, or building a knowledge base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Texts From Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Split the documents into chunks suitable for processing\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=2000, chunk_overlap=200)\n",
    "texts = python_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet deals with the preparation of documents for processing by splitting them into more manageable pieces:\n",
    "\n",
    "1. `RecursiveCharacterTextSplitter`: This is a class from the `langchain` library, specifically from the `text_splitter` module. Its purpose is to split long texts into smaller chunks that are easier to process by language models, which often have a maximum token or character limit.\n",
    "\n",
    "2. `RecursiveCharacterTextSplitter.from_language`: This method initializes the text splitter with settings appropriate for a specific programming language. In this case:\n",
    "   - `language=Language.PYTHON`: This specifies that the text being split is Python code. This is important because the way you split Python code may differ from how you'd split natural language or code in another programming language due to syntax and structural considerations.\n",
    "   - `chunk_size=2000`: This parameter defines the size of each chunk in characters. It's set to 2000 characters, meaning each chunk of text will be at most 2000 characters long.\n",
    "   - `chunk_overlap=200`: This setting allows for an overlap of 200 characters between consecutive chunks. Overlapping can be helpful for ensuring that no contextual information is lost at the boundaries of each chunk, which can be particularly important for tasks like training machine learning models or running analyses that depend on context.\n",
    "\n",
    "3. `texts = python_splitter.split_documents(documents)`: After setting up the text splitter, this line actually performs the splitting operation. It takes the previously loaded documents (`documents`) and splits each one into smaller chunks (`texts`). The resulting `texts` variable is a list of text chunks ready for further processing.\n",
    "\n",
    "By splitting the documents into smaller chunks, you make them compatible with language models that have a maximum input length, enabling you to process, analyze, or feed these chunks into such models sequentially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 375\n",
      "Chunk 1: Your very long text goes hereYour very long text goes hereYour very long text goes hereYour very\n",
      "Chunk 2: goes hereYour very long text goes hereYour very long text goes hereYour very long text goes\n",
      "Chunk 3: very long text goes hereYour very long text goes hereYour very long text goes hereYour very long\n",
      "Chunk 4: hereYour very long text goes hereYour very long text goes hereYour very long text goes hereYour\n",
      "Chunk 5: text goes hereYour very long text goes hereYour very long text goes hereYour very long text goes\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Assume we have a very long text that we need to split\n",
    "very_long_text = \"Your very long text goes here\" * 1000  # This creates a long string\n",
    "\n",
    "# Initialize the text splitter with a small chunk size to demonstrate the splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,       # Set chunk size to 100 characters\n",
    "    chunk_overlap=20,     # Set overlap to 20 characters\n",
    "    length_function=len,  # Function to measure chunk size by number of characters\n",
    "    is_separator_regex=False,  # The separators are not regex patterns\n",
    ")\n",
    "\n",
    "# Split the very long text into chunks using split_text\n",
    "chunks = text_splitter.split_text(very_long_text)\n",
    "\n",
    "# Print out the number of chunks and the first 100 characters of each chunk to demonstrate\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks[:5]):  # Print first 5 chunks for brevity\n",
    "    print(f\"Chunk {i+1}: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Vector Store and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "import config\n",
    "from importlib import reload\n",
    "reload(config)\n",
    "\n",
    "# Now using the updated import path for OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "api_key = config.OPEN_AI_KEY\n",
    "\n",
    "# Create a vector store from the documents and initialize the retriever\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=(), openai_api_key=api_key))\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates the integration of updated OpenAI embeddings with the LangChain library for creating a semantic search vector store. Here's a breakdown of what each part does:\n",
    "\n",
    "1. **Configuration and Dependencies:**\n",
    "   - `import config`: This imports a configuration file, presumably containing settings and API keys needed for the project. Specifically, it's used here to access the OpenAI API key.\n",
    "   - `from langchain_openai import OpenAIEmbeddings`: Imports the `OpenAIEmbeddings` class from the `langchain_openai` package. This class is responsible for generating embeddings using OpenAI's models, which are vector representations of text that capture semantic meaning.\n",
    "   - `from langchain.vectorstores import Chroma`: Imports `Chroma`, a vector store from the `langchain` library. Vector stores are databases optimized for storing and querying vector embeddings, facilitating fast and efficient semantic searches.\n",
    "\n",
    "2. **Reloading the Configuration:**\n",
    "   - `from importlib import reload; reload(config)`: This ensures that any changes made to the `config.py` file are updated in the current session. It's particularly useful in a development environment where the configuration may change.\n",
    "\n",
    "3. **API Key:**\n",
    "   - `api_key = config.OPEN_AI_KEY`: Retrieves the OpenAI API key from the `config` module, which is necessary for authenticating requests to OpenAI's API for generating embeddings.\n",
    "\n",
    "4. **Creating a Vector Store with Document Embeddings:**\n",
    "   - `db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=(), openai_api_key=api_key))`: This line initializes a `Chroma` vector store with embeddings of the documents in `texts`. Each document is embedded using `OpenAIEmbeddings`, which calls OpenAI's API using the provided API key. The `disallowed_special=()` argument specifies that no special tokens are excluded from the embeddings, though this can be customized if needed.\n",
    "\n",
    "5. **Initializing the Retriever for Semantic Search:**\n",
    "   - `retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})`: This initializes a retriever on the vector store `db` for semantic search. The `search_type=\"mmr\"` indicates that the Maximal Marginal Relevance algorithm is used to retrieve search results. This algorithm is designed to balance relevance and diversity in the results, making it particularly useful for ensuring a broad overview of documents related to a query. The `search_kwargs={\"k\": 8}` specifies that the top 8 most relevant documents are returned for a given query.\n",
    "\n",
    "Overall, this code sets up a system for semantically searching through a collection of documents by converting them into vector embeddings and storing them in a `Chroma` vector store. The `OpenAIEmbeddings` class is used to generate these embeddings, leveraging OpenAI's powerful language models. The result is a flexible and powerful tool for finding documents that are semantically similar to a given query, which can be applied in various contexts, such as building recommendation systems, conducting research, or creating intelligent chatbots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Chat Model and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from importlib import reload\n",
    "reload(config)\n",
    "\n",
    "# Initialize the chat model, memory, and the conversational retrieval chain\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=api_key)  # Replace \"gpt-4\" with the model you are using\n",
    "memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet integrates several components from the LangChain library to set up a conversational AI system powered by OpenAI's GPT-4 model. The system is capable of understanding context, retaining conversation history, and retrieving relevant information from a vector store. Here's a step-by-step breakdown:\n",
    "\n",
    "1. **Configuration and Module Reloading:**\n",
    "   - The code begins by importing a `config` module, which presumably contains necessary configuration variables such as API keys.\n",
    "   - The `reload` function from the `importlib` module is used to reload the `config` module. This ensures that any updates to the configuration (like changing the API key in `config.py`) are immediately reflected without needing to restart the interpreter or kernel.\n",
    "\n",
    "2. **Import LangChain Components:**\n",
    "   - `from langchain.chat_models import ChatOpenAI`: Imports the `ChatOpenAI` class, which is designed to facilitate interaction with OpenAI's chat models (like GPT-3.5 or GPT-4) for generating conversational responses.\n",
    "   - `from langchain.memory import ConversationSummaryMemory`: Imports the `ConversationSummaryMemory` class, a utility for tracking the history of a conversation. This is crucial for context-aware conversational agents that need to remember what has been said previously.\n",
    "   - `from langchain.chains import ConversationalRetrievalChain`: Imports the `ConversationalRetrievalChain` class, which combines conversational models with retrieval capabilities to answer queries based on a body of knowledge (like documents stored in a vector store).\n",
    "\n",
    "3. **Initialize the Chat Model:**\n",
    "   - `llm = ChatOpenAI(model_name=\"gpt-4\", openai_api_key=api_key)`: Initializes an instance of the `ChatOpenAI` class, specifying `gpt-4` as the model to use. The `api_key` variable (obtained from the `config` module) is used for authentication.\n",
    "\n",
    "4. **Initialize Memory for Conversation History:**\n",
    "   - `memory = ConversationSummaryMemory(llm=llm, memory_key=\"chat_history\", return_messages=True)`: Creates an instance of `ConversationSummaryMemory`. This memory component is associated with the previously initialized chat model (`llm`) and is configured to track and return messages related to the key \"chat_history\". This enables the system to maintain context over the course of a conversation.\n",
    "\n",
    "5. **Set Up the Conversational Retrieval Chain:**\n",
    "   - `qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)`: This line combines the chat model, the memory component, and a previously initialized `retriever` into a `ConversationalRetrievalChain`. The `retriever` is responsible for pulling relevant information from a vector store (initialized in earlier steps) based on the current conversational context. This chain allows the AI to not only generate conversational responses but also retrieve and incorporate specific information from a knowledge base, making the conversation more informative and contextually relevant.\n",
    "\n",
    "The result of this setup is a sophisticated conversational AI system that can maintain the context of a conversation, remember what has been discussed, and access a vast store of information to answer questions accurately. This is especially useful for applications like virtual assistants, educational bots, and any system requiring nuanced human-AI interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask a Question and Retrieve Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/github-RAGchain/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The `train.py` file is responsible for setting up and training the Generative Adversarial Network (GAN). Specifically, it does the following:\\n\\n1. It initializes the start time of the training session.\\n2. It creates instances of the `Discriminator` and `Generator` classes for both the `Sprite` and `3D` models, and places them on the specified device.\\n3. It sets up the Adam optimizers for both the discriminators and the generators.\\n4. It sets up the Loss functions (`L1Loss` and `MSELoss`).\\n5. If the `LOAD_MODEL` flag is set in the config, it loads checkpoint files for both the generators and discriminators.\\n6. It creates instances of the `PokemonDataset` for both training and validation, and sets up the corresponding data loaders.\\n7. It initializes scalers for gradient scaling, which can be used to prevent gradient underflow during mixed precision training.\\n8. It contains code to convert various loss values from tensors to floats, if necessary.\\n9. It writes training statistics, such as epoch number, loss values, learning rate, etc., to a CSV file for later analysis.\\n10. Besides, it also includes a method `test()` to test the `Discriminator` model.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question about the content of the repository\n",
    "question = \"What is the role of train.py file?\"\n",
    "result = qa(question)\n",
    "result['answer']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to use the previously set up conversational AI system to ask a question about the content of a repository and retrieve a relevant answer. Here's a detailed breakdown:\n",
    "\n",
    "1. **Asking a Question:**\n",
    "   - `question = \"What is the role of train.py file?\"`: This line sets up a question as a string. In this context, the question is about understanding the purpose or role of a specific file (`train.py`) within the repository. This kind of question is typical when navigating large codebases, making this system particularly useful for developers and analysts seeking quick insights into unfamiliar repositories.\n",
    "\n",
    "2. **Retrieving an Answer:**\n",
    "   - `result = qa(question)`: This line leverages the `qa` object, which is an instance of `ConversationalRetrievalChain`, to process the question. The `ConversationalRetrievalChain` utilizes the integrated language model (in this case, GPT-4), the conversation memory to maintain context, and the retrieval system that can pull relevant information from the document corpus stored in the `Chroma` vector store. The system formulates an answer by considering the context of the conversation, previous messages, and the content of the repository as understood through the documents in the vector store.\n",
    "\n",
    "3. **Accessing the Answer:**\n",
    "   - `result['answer']`: This line accesses the 'answer' key of the `result` dictionary returned by the `qa` call. The value associated with this key is the system's response to the question posed. It represents the AI's best effort to provide a meaningful answer based on its current knowledge and the conversational context.\n",
    "\n",
    "The overall flow here demonstrates a practical application of conversational AI in navigating and extracting insights from complex information repositories, such as software codebases. By asking specific questions, users can quickly gain understanding or clarification about parts of the repository without manually searching through files, making it a powerful tool for learning and collaboration in software development and beyond.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI:  This repository contains the code for an application that uses a generative model to convert uploaded images into a Pokémon-like style. The model is trained on a dataset of Pokémon images, both 3D and sprite-based, using a CycleGAN architecture.\n",
      "\n",
      "The code includes various classes and functions for handling the dataset, defining the model architecture (including the generator and discriminator networks), applying image transformations, and saving/loading model checkpoints. There is also a function for creating a study to optimize the model's hyperparameters.\n",
      "\n",
      "In addition, there is a script for processing images with the trained model: it loads the generator network, applies the necessary transformations to the input images, generates the Pokémon-style images, and saves the results to an output directory.\n",
      "\n",
      "All of this is orchestrated by a main script that sets everything up and starts the training process. The code also logs various metrics during training, such as losses and hyperparameters, and saves the input and output image paths for each training session.\n",
      "AI:  The code provided seems to be part of a machine learning model training script and it uses Optuna, a Python library for hyperparameter optimization. However, it's not a full script for `optuna.py`. Here's a brief explanation of the Optuna related parts:\n",
      "\n",
      "1. `study = optuna.create_study(direction='minimize')`: This line creates a study object. Optuna's optimization is done on a \"study\" level, where a study represents an optimization session. The direction 'minimize' means that the goal of optimization is to find the minimum value of the objective function.\n",
      "\n",
      "2. `study.optimize(objective, n_trials=50)`: This line starts the optimization. The number of trials to run is set to 50. In each trial, Optuna suggests hyperparameters and the objective function is calculated. Optuna's aim is to find the hyperparameters which minimize the objective function.\n",
      "\n",
      "3. `trial.report(val_loss, epoch)`: This reports the intermediate value at each step. It is useful for pruning unpromising trials.\n",
      "\n",
      "4. `if trial.should_prune(): raise optuna.exceptions.TrialPruned()`: This stops the current trial early if it is not promising (based on intermediate values reported).\n",
      "\n",
      "5. `trial = study.best_trial`, `trial.value`, `trial.params.items()`: These lines retrieve the best trial information, including the best value found and the parameters that led to that value.\n",
      "\n",
      "The goal of using Optuna in this script is to find the best hyperparameters that minimize the validation loss (`val_loss`) of the model.\n",
      "AI:  The data for training is parsed using the PokemonDataset class. This class takes in the paths to the 3D Pokemon and Sprite Pokemon images. For each image, it opens the image file, converts it to RGB to ensure 3 colors channels, and applies a transformation if any are specified. The transformation pipeline includes resizing the image to 256x256 pixels, flipping the image horizontally 50% of the time, normalizing the pixel values to have a mean of 0.5 and a standard deviation of 0.5, and converting the image to a PyTorch tensor. The preprocessed images are then returned and ready for training. The DataLoader function from PyTorch is then used to create a data loader with a specified batch size and shuffling option.\n",
      "Exiting chat...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Ask the user for input\n",
    "    user_input = input(\"You: \")\n",
    "    \n",
    "    # Check if the user wants to quit\n",
    "    if user_input.lower() in ['exit', 'quit']:\n",
    "        print(\"Exiting chat...\")\n",
    "        break\n",
    "    \n",
    "    # Use the AI to generate a response\n",
    "    try:\n",
    "        result = qa(user_input)\n",
    "        print(\"AI: \", result['answer'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
